import os
import numpy as np
import pandas as pd
import seaborn as sns
import shap
import matplotlib.pyplot as plt
from rich import print
from hyperopt import fmin, tpe, hp, STATUS_OK, Trials

from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, 
                             matthews_corrcoef, confusion_matrix, roc_curve, auc, 
                             precision_recall_curve)
from sklearn import metrics
from sklearn.inspection import permutation_importance

from lightgbm import LGBMClassifier
import lightgbm as lgbm
from xgboost import XGBClassifier
import xgboost as xgb

from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import Runnable

#from sklearn.linear_model import LogisticRegression
#from sklearn.neighbors import KNeighborsClassifier
#from sklearn.naive_bayes import GaussianNB
#from sklearn.svm import SVC
 
#ê° ëª¨ë¸ì˜ í‰ê°€ ì§€í‘œë¥¼ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜
#ì •í™•ë„,ì •ë°€ë„,ì¬í˜„ìœ¨,,F1 ì ìˆ˜, mcc, í˜¼ë™ í–‰ë ¬, AUC-ROC,PR-AUC

def model_eval(model_object, model_name: str, X_test, y_test):
    # 1. ê¸°ë³¸ ì˜ˆì¸¡ (ì„ê³„ì¹˜ 0.5)
    prediction = model_object.predict(X_test)

    # 2. ìµœì  ì„ê³„ì¹˜ íƒìƒ‰ 
    best_threshold = 0.5
    best_f1 = f1_score(y_test, prediction) 
    optimal_preds = prediction 
    y_pred_proba = None

    # predict_probaê°€ ìˆëŠ”ì§€ í™•ì¸
    if hasattr(model_object, "predict_proba"):
        y_pred_proba = model_object.predict_proba(X_test)[:, 1]
        
        # 0.01ë¶€í„° 0.49ê¹Œì§€ 0.01 ê°„ê²©ìœ¼ë¡œ ì„ê³„ì¹˜ë¥¼ í…ŒìŠ¤íŠ¸
        thresholds = np.arange(0.01, 0.5, 0.01)
        
        for threshold in thresholds:
            threshold = round(threshold, 2)
            
            preds = (y_pred_proba >= threshold).astype(int)
            
            f1 = f1_score(y_test, preds)
            
            if (f1 > best_f1):
                best_f1 = f1
                best_threshold = threshold

        optimal_preds = (y_pred_proba >= best_threshold).astype(int)
        
        print(f"   > ìµœì  ì„ê³„ì¹˜: {best_threshold:.2f} (F1-Score: {best_f1:.3f})")
    
    else:
        print("--- (predict_probaê°€ ì—†ì–´ ìµœì  ì„ê³„ì¹˜ íƒìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.) ---")

    metrics_funcs = {
        'ì •í™•ë„ (Accuracy)': accuracy_score,
        'ì •ë°€ë„ (Precision)': precision_score,
        'ì¬í˜„ìœ¨ (Recall)': recall_score,
        'F1 ì ìˆ˜ (F1-Score)': f1_score,
        'ë§¤íŠœ ìƒê´€ ê³„ìˆ˜ (MCC)': matthews_corrcoef
    }
    
    results = {}
    
    # ì„ê³„ì¹˜ ë³€ê²½ ì „ (Default)
    default_metrics = {name: func(y_test, prediction) for name, func in metrics_funcs.items()}
    results['Default (Threshold: 0.5)'] = default_metrics
    
    # ì„ê³„ì¹˜ ë³€ê²½ í›„ (Optimal) - ì„ê³„ê°’ì´ 0.5ê°€ ì•„ë‹ ë•Œë§Œ ì¶”ê°€
    if best_threshold != 0.5:
        optimal_metrics = {name: func(y_test, optimal_preds) for name, func in metrics_funcs.items()}
        results[f'Optimal (Threshold: {best_threshold:.2f})'] = optimal_metrics

    results_df = pd.DataFrame(results).round(4)
    display(results_df) 
    
    
    plt.rcParams['font.family'] = 'NanumGothic'
    plt.rcParams['axes.unicode_minus'] = False

    
    fig, axes = plt.subplots(2, 2, figsize=(12, 9))
    fig.suptitle(f'{model_name} ëª¨ë¸ í‰ê°€', fontsize=18)

    # --- í˜¼ë™ í–‰ë ¬ (Default: 0.5) ---
    conf_matrix_default = confusion_matrix(y_test, prediction, labels=[1, 0])
    sns.heatmap(conf_matrix_default, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0],
                xticklabels=['íì—…(1)', 'ìš´ì˜ ì¤‘(0)'],
                yticklabels=['íì—…(1)', 'ìš´ì˜ ì¤‘(0)'])
    axes[0, 0].set_aspect('equal')
    axes[0, 0].set_title('Confusion Matrix (Default: 0.5)', fontsize=14)
    axes[0, 0].set_xlabel('Predicted Label')
    axes[0, 0].set_ylabel('True Label')

    # --- ìˆ˜ì •ëœ ì„ê³„ì¹˜ê°€ ì ìš©ëœ í˜¼ë™ í–‰ë ¬ (Optimal) ---
    if best_threshold != 0.5:
        conf_matrix_optimal = confusion_matrix(y_test, optimal_preds, labels=[1, 0])
        sns.heatmap(conf_matrix_optimal, annot=True, fmt='d', cmap='Oranges', ax=axes[0, 1],
                    xticklabels=['íì—…(1)', 'ìš´ì˜ ì¤‘(0)'],
                    yticklabels=['íì—…(1)', 'ìš´ì˜ ì¤‘(0)'])
        axes[0, 1].set_aspect('equal')
        axes[0, 1].set_title(f'Confusion Matrix (Optimal: {best_threshold:.2f})', fontsize=14)
        axes[0, 1].set_xlabel('Predicted Label')
        axes[0, 1].set_ylabel('True Label')
    else:
        # ìµœì  ì„ê³„ì¹˜ê°€ 0.5ì´ê±°ë‚˜ ì°¾ì§€ ëª»í•œ ê²½ìš°
        axes[0, 1].axis('off')
        axes[0, 1].text(0.5, 0.5, 'Optimal Threshold is 0.5\n(or N/A)', 
                       ha='center', va='center', fontsize=12, wrap=True)

    # --- ROC & PR ì»¤ë¸Œ ---
    if y_pred_proba is not None:
        # ROC-AUC
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
        roc_auc = auc(fpr, tpr)
        axes[1, 0].plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
        axes[1, 0].plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')
        axes[1, 0].set_title('ROC Curve', fontsize=14)
        axes[1, 0].legend(loc="lower right")

        # PR-AUC
        prec, rec, _ = precision_recall_curve(y_test, y_pred_proba)
        pr_auc = auc(rec, prec)
        axes[1, 1].plot(rec, prec, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.2f})')
        axes[1, 1].set_title('Precision-Recall Curve', fontsize=14)
        axes[1, 1].legend(loc="lower left")
    else:
        # predict_probaê°€ ì—†ëŠ” ê²½ìš°
        axes[1, 0].axis('off')
        axes[1, 0].text(0.5, 0.5, 'ROC Curve not available\n(no predict_proba)', 
                       ha='center', va='center', fontsize=12)
        axes[1, 1].axis('off')
        axes[1, 1].text(0.5, 0.5, 'PR Curve not available\n(no predict_proba)', 
                       ha='center', va='center', fontsize=12)

    plt.subplots_adjust(top=0.9, hspace=0.3)
    plt.show()

    return best_threshold

class Boost_model:
    def __init__(self,model_type: str,train_data,GPU ='off',
                 early_stopping_rounds = 50,n_estimators=500,learning_rate=0.05,
                 min_split_gain=0.05, random_state=42, subsample=0.8,colsample_bytree=0.8,cv_number = 5):
        
        self.train_data = train_data
        self.model_type = model_type
        self.early_stopping_rounds = early_stopping_rounds
        self.n_estimators = n_estimators
        self.learning_rate= learning_rate
        self.subsample = subsample
        self.colsample_bytree = colsample_bytree
        self.random_state = random_state
        self.min_split_gain = min_split_gain
        self.scale_pos_weight = (len(self.train_data.y_train) - sum(self.train_data.y_train)) / sum(self.train_data.y_train)
        self.check_point = 0
        self.GPU = GPU
        self.cv_number = cv_number  # íŠœë‹ ì‹œ ì‚¬ìš©í•  CV í´ë“œ ìˆ˜
        self.best_params = None # íŠœë‹ ê²°ê³¼ë¥¼ ì €ì¥í•  ë³€ìˆ˜
        self.model = None
        self.pred_proba = None #í…ŒìŠ¤íŠ¸ ê²°ê³¼ì—ì„œ ìœ„í—˜ ì ìˆ˜ ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ë³€ìˆ˜
        self.best_threshold = None #ìµœì ì˜ ì„ê³„ê°’
        self.emergency_mc = None #ìœ„í—˜ ê°€ë§¹ì  

        if self.model_type == 'lgbm':
            self.title = "Light Gradient Boosting Machine"
        elif self.model_type == 'xgb':
            self.title = "Exreme Gradient Boosting"
        else:
             raise ValueError("ì •í™•í•œ ë¶€ìŠ¤íŠ¸ ëª¨ë¸ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”(lgbm,xgb)")

        default_params = {
        'n_estimators': self.n_estimators,
        'learning_rate': self.learning_rate,
        'subsample': self.subsample,
        'colsample_bytree': self.colsample_bytree,
        'random_state': self.random_state}

        self.model = self._get_model(default_params)

    def _define_search_space(self):
            if self.model_type == 'xgb':
                return {
                    'n_estimators': hp.quniform('n_estimators', 100, 1000, 100),
                    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),
                    'max_depth': hp.quniform('max_depth', 3, 15, 1),
                    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),
                    'subsample': hp.uniform('subsample', 0.6, 1.0),
                    'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1.0),
                    'gamma': hp.uniform('gamma', 0, 0.5),
                    'reg_alpha': hp.uniform('reg_alpha', 0, 1), 
                    'reg_lambda': hp.uniform('reg_lambda', 0, 1)      
                }
                
            elif self.model_type == 'lgbm':
                return {
                    'n_estimators': hp.quniform('n_estimators', 100, 2000, 100),
                    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),
                    'num_leaves': hp.quniform('num_leaves', 20, 150, 1),
                    'max_depth': hp.quniform('max_depth', 3, 15, 1),
                    'min_child_samples': hp.quniform('min_child_samples', 20, 100, 5),
                    'subsample': hp.uniform('subsample', 0.6, 1.0),
                    'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1.0),
                    'reg_alpha': hp.uniform('reg_alpha', 0, 1), 
                    'reg_lambda': hp.uniform('reg_lambda', 0, 1) 
                }
            else:
                raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ ì´ë¦„ì…ë‹ˆë‹¤.")

    def _get_model(self, params):
            
            model_params = params.copy() 
            
            if self.model_type == 'xgb':
                for p in ['n_estimators', 'max_depth', 'min_child_weight']:
                    if p in model_params:
                        model_params[p] = int(model_params[p])
                
                
                model_params['scale_pos_weight'] = self.scale_pos_weight
                model_params['random_state'] = self.random_state
                model_params['n_jobs'] = -1
                model_params['use_label_encoder'] = False
                model_params['early_stopping_rounds'] = self.early_stopping_rounds

                if self.GPU == 'on':
                    model_params['device'] = 'cuda'
                    model_params['tree_method'] = 'hist'

                return XGBClassifier(**model_params)

            elif self.model_type == 'lgbm':
                for p in ['n_estimators', 'num_leaves', 'max_depth', 'min_child_samples']:
                    if p in model_params:
                        model_params[p] = int(model_params[p])

                
                model_params['class_weight'] = 'balanced' 
                model_params['random_state'] = self.random_state
                model_params['n_jobs'] = -1
                model_params['verbose'] = -1

                if 'min_split_gain' not in model_params:
                     model_params['min_split_gain'] = self.min_split_gain

                if self.GPU == 'on':
                    model_params['device'] = 'cuda'

                return LGBMClassifier(**model_params)        

    def _objective(self, params):
            
            model = self._get_model(params)
            
            f1_scores_list= []
            
            skf = StratifiedKFold(n_splits=self.cv_number, shuffle=True, random_state=42)
        
            for tr_index, val_index in skf.split(self.train_data.X_train, self.train_data.y_train):
            
                X_tr, X_val = self.train_data.X_train.iloc[tr_index], self.train_data.X_train.iloc[val_index]
                y_tr, y_val = self.train_data.y_train.iloc[tr_index], self.train_data.y_train.iloc[val_index]

                if self.model_type == 'xgb':
                    model.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr), (X_val, y_val)], verbose=False)
                
                elif self.model_type == 'lgbm':
                    model.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr), (X_val, y_val)],
                        callbacks=[lgbm.early_stopping(stopping_rounds=self.early_stopping_rounds, verbose=False)])

                preds = model.predict(X_val)
                f1_scores_list.append(f1_score(y_val, preds))
            
            score = np.mean(f1_scores_list)

            return {'loss': -score, 'status': STATUS_OK}                    

    def _tune(self, max_evals=50):
            
            print(f"--- ìµœì í™” ê¸°ì¤€: êµì°¨ ê²€ì¦ (CV={self.cv_number}) F1 ì ìˆ˜ ---")
            space = self._define_search_space()
            trials = Trials()
        
            best_params_from_fmin = fmin(
                fn=self._objective,
                space=space,
                algo=tpe.suggest,
                max_evals=max_evals,
                trials=trials,
                rstate=np.random.default_rng(42)
            )
        
            self.best_params = best_params_from_fmin
            
            print("######## ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ########")
            print(self.best_params)
            print("\n")   

    def fit(self):
        if self.model is None:
            raise ValueError("ì •í™•í•œ ë¶€ìŠ¤íŠ¸ ëª¨ë¸ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”(lgbm,xgb)")
    
        elif self.model_type == 'lgbm':
            self.model.fit(
            self.train_data.X_tr, self.train_data.y_tr,
            eval_set=[(self.train_data.X_tr, self.train_data.y_tr), (self.train_data.X_val, self.train_data.y_val)]
            ,callbacks=[lgbm.early_stopping(stopping_rounds=self.early_stopping_rounds, verbose=-1)])

        elif self.model_type == 'xgb':
            self.model.fit(
            self.train_data.X_tr, self.train_data.y_tr,
            eval_set=[(self.train_data.X_tr, self.train_data.y_tr), (self.train_data.X_val, self.train_data.y_val)], verbose=False)
        
        self.check_point += 1
        self.pred_proba = self.model.predict_proba(self.train_data.X_test)[:, 1]

    def evaluation(self):
        self.fit()
        if self.model is None:
            raise ValueError("ì •í™•í•œ ë¶€ìŠ¤íŠ¸ ëª¨ë¸ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”(lgbm,xgb)")
        
        self.best_threshold =  model_eval(self.model,self.title,self.train_data.X_test,self.train_data.y_test)
        optimal_preds = (self.pred_proba >= self.best_threshold).astype(int)
        self.emergency_mc = np.where(optimal_preds == 1)[0]

    def Tuner(self, max_evals=50):
            # 1. íŠœë‹ ì‹¤í–‰
            self._tune(max_evals=max_evals)
            
            # 2. íŠœë‹ëœ íŒŒë¼ë¯¸í„°ë¡œ self.model êµì²´
            self.model = self._get_model(self.best_params)
            
            # 3. í‰ê°€ ì‹¤í–‰ (ë‚´ë¶€ì ìœ¼ë¡œ fit -> model_eval í˜¸ì¶œ)
            self.evaluation()    

    def plot_feature_importance(self,top_n=20):
        if self.check_point == 0:
            raise ValueError("ëª¨ë¸ì„ ë¨¼ì € í•™ìŠµí•´ì•¼ í•©ë‹ˆë‹¤")
        
        importances = self.model.feature_importances_
        features = self.train_data.X_val.columns

        idx = np.argsort(importances)[::-1][:top_n]
        plt.barh(np.array(features)[idx][::-1], np.array(importances)[idx][::-1])
        plt.title(f"{self.model_type} Feature Importance (Top {top_n})")
        plt.xlabel("Importance score")
        plt.ylabel("Features")
        plt.show()
    
    def permutation_importance_plot(self):
        if self.check_point == 0:
            raise ValueError("ëª¨ë¸ì„ ë¨¼ì € í•™ìŠµí•´ì•¼ í•©ë‹ˆë‹¤")

        result = permutation_importance(
            self.model, self.train_data.X_val, self.train_data.y_val,
            scoring="f1",
            n_repeats=1,
            random_state=42,
            n_jobs=-1)

        fi = pd.DataFrame({
            "feature": self.train_data.X_val.columns,
            "importance": result.importances_mean
        }).sort_values(by="importance", ascending=False)

        print(fi.head(10))

        top_features = fi.head(20)

        plt.figure(figsize=(8, 6))
        plt.barh(top_features["feature"], top_features["importance"], color="skyblue")
        plt.xlabel("Permutation Importance")
        plt.ylabel("Feature")
        plt.title(f"{self.model_type} Top 20 Features by Permutation Importance")
        plt.gca().invert_yaxis() 
        plt.show()

    def custom_threshold(self,my_threshold = 0.5): 
        custom_threshold_model_eval(self.model, self.title, self.train_data.X_test, self.train_data.y_test, my_threshold = my_threshold)  

class ShapAnalysis:
    """
    í•™ìŠµëœ íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸(XGBoost, LightGBM ë“±)ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼
    SHAPì„ ì´ìš©í•´ ë¶„ì„í•˜ê³  ì‹œê°í™”í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    """
    def __init__(self, boost_model, train_data):

        self.model = boost_model
        self.X_test = train_data.X_test
        self.y_test = train_data.y_test
        self.explainer = shap.TreeExplainer(self.model)
        
        # ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜
        self.single_data_point = None
        self.shap_values_single = None
        self.answer = None
        self.top5_positive = None
        self.probability_class_1 = None
        self.expected_value = self.explainer.expected_value

    def select_sample(self, index=0):
        
        self.single_data_point = self.X_test.iloc[[index]]
        self.answer = self.y_test.iloc[index]

        self.shap_values_single = self.explainer.shap_values(self.single_data_point)
        print(f"--- ë°ì´í„° ì¸ë±ìŠ¤ {index}ë²ˆ ìƒ˜í”Œì— ëŒ€í•œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤. ---")
        return self

    def text_summary(self):

        if self.single_data_point is None:
            raise ValueError("ë¨¼ì € `select_sample` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ë¶„ì„í•  ë°ì´í„°ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")

        self.probability_class_1 = self.model.predict_proba(self.single_data_point)[0, 1]
        answer_dic = {0: "ì •ìƒ", 1: "íì—… ìœ„ê¸°"}
        answer = answer_dic[self.answer]
        
        print("\n" + "="*60)
        print(f"ê°€ë§¹ì • í˜„í™©: {answer}({self.answer})")
        print(f"ğŸ¯ ê°€ë§¹ì ì´ íì—…í•  ì˜ˆì¸¡ í™•ë¥ : {self.probability_class_1:.3f}\n")
        print(f"ğŸ“Š ëª¨ë¸ì˜ í‰ê·  ì˜ˆì¸¡ ê¸°ì¤€ê°’ (Base Value): {self.expected_value[0]:.3f}\n")
        

        shap_df = pd.DataFrame({
            'Feature': self.single_data_point.columns,
            'SHAP Value (ê¸°ì—¬ë„)': self.shap_values_single.flatten()
        })
        
        positive_shap_df = shap_df[shap_df['SHAP Value (ê¸°ì—¬ë„)'] > 0]
        
        positive_shap_df = positive_shap_df.sort_values(by='SHAP Value (ê¸°ì—¬ë„)', ascending=False)
        self.top5_positive = positive_shap_df.head(5)

        print("íì—… ì˜ˆì¸¡ ìš”ì¸ TOP 5 í”¼ì³:")
        display(self.top5_positive)

        
        
    def LLM_summary(self):


        llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0,  
        )

        #í‘œ í˜•ì‹ì„ ê¸€ìë¡œ ë³€í™˜
        result_string = self.top5_positive.to_markdown(index=False) # ì¤‘ìš”ë„ top5
        X_test_string = self.single_data_point.to_markdown(index=False) # ê°€ê²Œ ë°ì´í„°
        table_string = result_string + X_test_string + f"ê°€ë§¹ì ì˜ íì—… ìœ„ê¸° ì§€ìˆ˜: {self.probability_class_1:.3f}\n"
        #í”„ë¡¬í”„íŠ¸
        prompt_template = """
        ë‹¹ì‹ ì€ ìœ„ê¸° ìƒí™©ì˜ ì†Œìƒê³µì¸ì„ ìœ„í•œ ìµœê³ ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ì²«ë²ˆì§¸ ë°ì´í„°ëŠ” íŠ¹ì • ê°€ê³„ì˜ íì—…ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì¸ê³¼ ê·¸ ì¤‘ìš”ë„(ê°€ì¤‘ì¹˜)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í‘œì…ë‹ˆë‹¤.
        ë‘ë²ˆì§¸ ë°ì´í„°ëŠ” í•´ë‹¹ ê°€ê³„ì˜ ë§¤ì¶œ ì§€í‘œë¥¼ ì˜ë¯¸í•˜ëŠ” ë°ì´í„°ì…ë‹ˆë‹¤.

        {table_data}

        ### ë¶„ì„ ê°€ì´ë“œë¼ì¸
        - ë‹¨ìˆœíˆ ê°€ì¤‘ì¹˜ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ ìš”ì¸ì„ ë‚˜ì—´í•˜ì§€ ë§ˆì„¸ìš”
        - ê°€ê³„ì˜ ë°ì´í„°ì™€ íì—… ìš”ì¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ëª¨ë‘ ê³ ë ¤í•´ì„œ ëŒ€ë‹µí•˜ì„¸ìš”
        - **ì„œë¡œ ì—°ê´€ëœ ìš”ì¸ë“¤ì€ í•˜ë‚˜ì˜ í•µì‹¬ ë¬¸ì œë¡œ ë¬¶ì–´ì„œ ì¢…í•©ì ìœ¼ë¡œ í•´ì„í•´ì•¼ í•©ë‹ˆë‹¤.** ì˜ˆë¥¼ ë“¤ì–´, 'ìµœê·¼ 3ê°œì›” ë§¤ì¶œ í•˜ë½'ê³¼ 'ìµœê·¼ 6ê°œì›” ë§¤ì¶œ í•˜ë½' ìš”ì¸ì´ í•¨ê»˜ ìˆë‹¤ë©´, ì´ëŠ” 'ì¥ê¸°ì ì¸ ë§¤ì¶œ ê°ì†Œ ì¶”ì„¸'ë¼ëŠ” í•˜ë‚˜ì˜ í†µí•©ëœ ì¸ì‚¬ì´íŠ¸ë¡œ ë¶„ì„í•´ì•¼ í•©ë‹ˆë‹¤.
        - ì£¼ìš” ìš”ì¸ì— ì˜ì—… ê°œì›”ì´ ìˆë‹¤ë©´ ì˜ì—… ê°œì›” ìì²´ê°€ íì—… ì›ì¸ì´ ì•„ë‹˜ì„ ëª…ì‹¬í•˜ì‹­ì‹œì˜¤
        - ì£¼ìš” ìš”ì¸ì— ì„ëŒ€ë£Œê°€ ìˆìœ¼ë©´ ê°€ê³„ì˜ ë§¤ì¶œ ì›ê°€ ì ˆê°ì„ ì œì•ˆí•˜ì„¸ìš”
        - ì—­ì„¸ê¶Œ ì ìˆ˜ëŠ” 300m ë‚´ì— ì§€í•˜ì²  ì—­ì´ ìˆìœ¼ë©´ +2ì  300~500m ì‚¬ì´ì— ì§€í•˜ì ˆ ì—­ì´ ìˆìœ¼ë©´ +1ì  ì…ë‹ˆë‹¤
        - ê°€ê³„ì˜ ìœ„ì¹˜ë¥¼ ì¬ë°°ì¹˜ í˜¹ì€ ìƒê¶Œ ì´ë™ë“±ì˜ ë‹¨ì‹œê°„ì— ë¹„ìš©ì´ ë§ì´ ë“¤ì–´ê°€ëŠ” ì¡°ì–¸ì€ ìì²´í•©ë‹ˆë‹¤
        - ì†Œìƒê³µì¸ ì…ì¥ì—ì„œ í•´ê²°í•  ìˆ˜ ìˆëŠ” í•´ê²°ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤

        ### ë‹¹ì‹ ì˜ ê³¼ì—…
        1.  **í•µì‹¬ ìœ„í—˜ ìš”ì•½**: í‘œë¥¼ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì¤‘ìš”í•œ 3ê°€ì§€ í•µì‹¬ ìš”ì¸ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ê³ , **í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½**í•´ì£¼ì„¸ìš”.
        2.  **ì†”ë£¨ì…˜ ì œì‹œ**: ìœ„ì—ì„œ ë¶„ì„í•œ í•µì‹¬ ìœ„í—˜ì„ ë°”íƒ•ìœ¼ë¡œ, ì‹¤í–‰ ê°€ëŠ¥í•œ **êµ¬ì²´ì ì¸ í•´ê²° ë°©ì•ˆ 3ê°€ì§€**ë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”.

        ### ê²°ê³¼ ì¶œë ¥ í˜•ì‹
        ê²°ê³¼ëŠ” ë‹¤ìŒ í˜•ì‹ì— ë§ì¶° ì‘ì„±í•´ì£¼ì„¸ìš”.

        ** ê°€ë§¹ì  ìœ„í—˜ ìš”ì¸ ë¶„ì„ **

        [ìœ„ê¸° ì§€ìˆ˜]: (ê°€ë§¹ì ì˜ íì—… ìœ„ê¸° ì§€ìˆ˜ë¥¼ ì†Œìˆ˜ì  3ìë¦¬ì—ì„œ ë°˜ì˜¬ë¦¼)

        [í•µì‹¬ ìœ„í—˜ ìš”ì•½]
        (ì—¬ê¸°ì— í•œ ë¬¸ì¥ ìš”ì•½)

        [ì†”ë£¨ì…˜ ì œì•ˆ]
        1. **ì†”ë£¨ì…˜ 1**: (ì²« ë²ˆì§¸ í•´ê²° ë°©ì•ˆê³¼ ê·¸ì— ëŒ€í•œ ì„¤ëª…)
        2. **ì†”ë£¨ì…˜ 2**: (ë‘ ë²ˆì§¸ í•´ê²° ë°©ì•ˆê³¼ ê·¸ì— ëŒ€í•œ ì„¤ëª…)
        3. **ì†”ë£¨ì…˜ 3**: (ì„¸ ë²ˆì§¸ í•´ê²° ë°©ì•ˆê³¼ ê·¸ì— ëŒ€í•œ ì„¤ëª…)
        """
        prompt = PromptTemplate(template=prompt_template,input_variables=["table_data"])
        chain = prompt | llm
        result = chain.invoke({"table_data": table_string})

        print('*********************ê°€ë§¹ì  ìœ„í—˜ ìš”ì†Œ ë¶„ì„*********************')
        print(result.content)

        
    def force_plot(self):
        
        if self.single_data_point is None:
            raise ValueError("ë¨¼ì € `select_sample` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ë¶„ì„í•  ë°ì´í„°ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
        shap.initjs()
        print("\n>>>Force Plot (ë‹¨ì¼ ë°ì´í„° ì˜ˆì¸¡ ì„¤ëª…)")
        display(shap.force_plot(
            self.expected_value[0],
            self.shap_values_single,
            self.single_data_point
        ))

    def summary_plot(self):
        
        print("\n>>>Summary Plot (ì „ì²´ íŠ¹ì„± ì¤‘ìš”ë„)")
        shap_values_all = self.explainer.shap_values(self.X_test)
        
        plt.figure(figsize=(10, 8))
        shap.summary_plot(shap_values_all, self.X_test, show=False)
        plt.title("SHAP Summary Plot", fontsize=14)
        plt.tight_layout()
        plt.show()

    def custom_bar_plot(self):

        if self.single_data_point is None:
            raise ValueError("ë¨¼ì € `select_sample` ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ë¶„ì„í•  ë°ì´í„°ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
        
        print("\n>>>Custom Bar Plot (íì—… ì˜ˆì¸¡ ê¸ì • ì˜í–¥ TOP 5)")
        
        top5_positive_plot = self.top5_positive.sort_values(by='SHAP Value (ê¸°ì—¬ë„)', ascending=True)
        colors = ['red'] * len(top5_positive_plot)
        
        plt.figure(figsize=(8, 5))
        plt.barh(top5_positive_plot['Feature'], top5_positive_plot['SHAP Value (ê¸°ì—¬ë„)'], color=colors)
        plt.title("íì—… ì˜ˆì¸¡ ê¸ì • ì˜í–¥ TOP 5", fontsize=16)
        plt.xlabel("SHAP Value (ê¸°ì—¬ë„)", fontsize=12)
        plt.grid(axis='x', linestyle='--', alpha=0.6)
        plt.axvline(x=0, color='black', linewidth=0.8)
        plt.tight_layout()
        plt.show()

    def single_sample_analysis(self,index=0):
        self.select_sample(index=index)
        self.text_summary()
        self.LLM_summary()
        self.custom_bar_plot()
        self.force_plot()

    def LLM_analysis(self,index=0):
        self.select_sample(index=index)
        self.text_summary()
        self.LLM_summary()  
